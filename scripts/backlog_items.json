[
  {
    "title": "PR template + orchestration docs",
    "body": "**Area:** Quality  \n**Priority:** Medium  \n### Acceptance Criteria\n- PR template enforces D/N/A markers, ≥70% coverage, ruff/black/mypy  \n- Docs page explaining slash-commands, runbook, Phoenix TZ note  \n### Notes\nImproves consistency and auditability.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Project Board mapping and snapshots",
    "body": "**Area:** DX  \n**Priority:** Medium  \n### Acceptance Criteria\n- Issues auto-added to Project v2 with Priority/Status mapping  \n- Commit `artifacts/backlog.json` and `artifacts/top_issues.json`  \n### Notes\nSet ORG_LOGIN + project number; verify fields.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Promotion guardrails (protected env + promote.yml)",
    "body": "**Area:** Security  \n**Priority:** High  \n### Acceptance Criteria\n- `promote.yml` targets protected `prod` env with required reviewers  \n- Preflight fails if required Human ACKs are missing  \n### Notes\nKeeps human gate on deploys; least-privilege enforced.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "LLM patcher stub with guardrails",
    "body": "**Area:** CI  \n**Priority:** Medium  \n### Acceptance Criteria\n- `scripts/llm_make_patch.py` outputs unified diff only; no secrets in logs  \n- Reject patches touching sensitive paths unless labeled for human review  \n### Notes\nProvider-agnostic; minimal context (trim logs) to manage token costs.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Metrics & budgets for CI Auto-Fix",
    "body": "**Area:** Process  \n**Priority:** Low  \n### Acceptance Criteria\n- Comment summary per PR: attempts, pass-rate, est. tokens  \n- Config for MAX_ATTEMPTS and optional spend cap; doc guidance  \n### Notes\nMeasure ROI before scaling beyond manual-trigger.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Pre-Commit Hooks] Ruff, Black, Mypy",
    "body": "**Description:**  \nAdd `.pre-commit-config.yaml` with ruff, black, and mypy hooks. Ensure contributors auto-format code.\n\n**Codex Prompt:**  \nCreate `.pre-commit-config.yaml`. Update `README.md` with installation instructions.  \n\n**Acceptance Criteria:**  \n- `pre-commit run --all-files` passes.  \n- Contributors auto-format with pre-commit.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Jira Webhook-based Sync (Epic)",
    "body": "The initial Jira webhook pipeline was introduced in **PR #80** with DynamoDB-backed storage. This Epic extends that work by making the pipeline production-grade: adding schema validation, idempotency, metrics, and developer documentation.\n\n### Acceptance Criteria\n- Webhook receiver (API Gateway + Lambda) remains deployed via CDK.\n- Signature/HMAC validation enforced for all incoming events.\n- Payloads normalized into schema `{issue_key, fields, changelog, updated_at, version}`.\n- Idempotent writes into DynamoDB (dedup by issue ID + updated timestamp).\n- Structured logging and retries re-use patterns from **PR #79**.\n- Metrics (success/failure counts, dedup events) published to CloudWatch.\n- Documentation updated to include webhook setup and troubleshooting.\n\n### Codex Prompt\nYou are a senior DevOps engineer and Python developer.  \n\n**Issue Reference: Epic: Jira Webhook-based Sync**  \n\nEnhance the Jira webhook ingestion pipeline introduced in PR #80.  \n\n**Core Tasks**\n1. Extend existing Lambda handler:\n   - Validate HMAC signatures (use shared secret from Secrets Manager).\n   - Enforce normalized schema and idempotent writes into DynamoDB.\n2. Add structured logging (building on #79).\n3. Add CloudWatch metrics for ingestion (processed, deduped, failed).\n4. Update CDK stack to enforce least-privilege IAM for DynamoDB, Secrets, and Logs.\n5. Write integration tests using sample webhook payloads.\n\n**Acceptance Criteria**\n- Valid Jira webhooks ingested into DynamoDB consistently.\n- Duplicate webhook deliveries do not produce duplicates.\n- CloudWatch metrics visible for success/failure.\n- Tests pass with recorded sample events.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Nightly Reconciliation (EventBridge) for Drift",
    "body": "PR #66 added optional EventBridge schedules, and PR #58 added CloudWatch alarms. This issue formalizes reconciliation: ensuring cached Jira data matches ground truth via JQL.\n\n### Acceptance Criteria\n- EventBridge nightly trigger runs Lambda reconciliation job.\n- Job compares sampled JQL results against DynamoDB entries.\n- Drift report generated: missing, outdated, inconsistent.\n- Auto-healing configurable (repair vs report only).\n- Metrics emitted (drift count, repairs attempted).\n- Alerts reused from PR #58 (CloudWatch alarms).\n\n### Codex Prompt\nYou are a senior SRE specializing in AWS.  \n\n**Issue Reference: Nightly Reconciliation (EventBridge) for Drift**  \n\nImplement a nightly drift reconciliation between Jira and the DynamoDB cache.  \n\n**Core Tasks**\n1. Add CDK EventBridge rule to run nightly (extend PR #66).\n2. Lambda function:\n   - Fetch JQL slice from Jira.\n   - Compare with cached items in DynamoDB.\n   - Generate drift report.\n   - Optionally repair drift.\n3. Publish metrics (drift detected, drift healed).\n4. Integrate CloudWatch alarms from PR #58.\n\n**Acceptance Criteria**\n- Nightly drift report generated.\n- Cache auto-heals if enabled.\n- Alerts fire if drift exceeds threshold.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Implement Manual CSV Fallback for Failed JQL Query",
    "body": "The current audit command relies on a direct API call to Jira to retrieve issues via JQL. We've experienced intermittent failures with this endpoint due to known Atlassian bugs (e.g., JRACLOUD-94876), which can cause the entire audit process to fail.\n\nTo improve the tool's resilience and ensure audits can always be completed, we need to implement a fallback mechanism. If the initial Jira JQL query fails after all retries, the system should prompt the user for a path to a manually exported Jira issues CSV file. The process should then continue by loading the issues from this CSV, bypassing the failed API call.\n\nThis feature is crucial for maintaining the reliability and usability of the ReleaseCopilot tool in production environments.\n\n### Acceptance Criteria\nAPI Failure Detection: The system must detect a failed Jira JQL query (e.g., a 400 Bad Request error) after exhausting its retry attempts.\n\nUser Prompt: Upon failure, the CLI must output a clear, actionable message to the user, explaining the failure and prompting for a file path to a Jira issues CSV export.\n\nCSV Loading: The system must be able to read and parse the provided CSV file, extracting the same Jira issue data that a successful API call would have returned.\n\nWorkflow Continuity: The audit process must continue from the point of failure, using the data from the CSV as if it were returned by the Jira API.\n\nError Handling: If the user provides an invalid file path or a malformed CSV, the system must display an informative error message and terminate gracefully.\n\n### Codex Prompt\nYou are a senior DevOps engineer and an expert Python developer.\n\nIssue Reference: Implement Manual CSV Fallback for Failed JQL Query\n\nYour task is to implement a robust fallback mechanism for the Jira JQL query within the audit command. The goal is to ensure the tool can complete a release audit even when the Jira API is unreliable.\n\nCore Task\nModify the Jira data retrieval logic in audit_from_config.py. The updated workflow should follow these steps:\n\nAttempt to fetch Jira issues using the standard JQL query with the existing retry mechanism.\n\nIf the query fails after all retries, log the error and transition to the fallback workflow.\n\nPrompt the user via the CLI for a local file path to a manually exported CSV file containing the Jira issues.\n\nLoad and parse the issue data from the provided CSV.\n\nContinue the rest of the audit process using the data from the CSV as the source of truth for Jira issues.\n\nImplementation Details\nPrompting: Use a clear, user-friendly prompt.\n\nError Handling: Implement robust error handling for file not found, permission errors, and invalid CSV format.\n\nCode Location: The core logic should be implemented within the audit_from_config.py file, likely within the main function or a new helper function that wraps the Jira API call.\n\nAcceptance Criteria\nThe feature will be considered complete when the following conditions are met:\n\nThe system detects a failed Jira JQL query after exhausting retries.\n\nThe user is prompted for a CSV file path via the CLI.\n\nThe system successfully loads and parses a valid Jira issues CSV file.\n\nThe audit process continues correctly using the CSV data.\n\nThe system handles invalid file paths or malformed CSVs by exiting gracefully with an informative error.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Fetch GitHub issues by label and export structured JSON snapshot",
    "body": "## Summary\r\nImplement a feature that pulls GitHub issues filtered by label and writes a structured JSON artifact that downstream automation (and LLM agents) can consume. This should integrate with the existing tooling conventions in `scripts/` and reuse the GitHub CLI authentication context.\r\n\r\n## Motivation\r\n- Give release automation access to label-scoped issue snapshots for reporting and changelog generation.\r\n- Provide machine-readable context files that align with the `context/` and `reports/` directories described in the project documentation.\r\n- Reduce manual effort required to answer “what’s in scope for label X” during triage.\r\n\r\n## Scope & Approach\r\n1. **Discovery**\r\n   - Review current GitHub integration utilities (e.g., `scripts/`, `clients/`) to determine the best place for the new feature.\r\n   - Confirm configuration patterns (YAML/JSON/env vars) for storing label lists and output paths.\r\n\r\n2. **Implementation**\r\n   - Add a script or module function that accepts one or more labels, fetches matching GitHub issues, and normalizes the results into a deterministic JSON schema with metadata and issue fields.\r\n   - Persist one JSON file per label under `data/issues/` (or configurable path) and ensure directories are created as needed.\r\n   - Include logging and error handling for rate limits or missing labels.\r\n\r\n3. **Testing & Validation**\r\n   - Provide unit or integration tests (mocked responses) verifying label filtering, JSON structure, and empty results handling.\r\n   - Update documentation with usage instructions.\r\n\r\n## Acceptance Criteria\r\n- Running the new command with sample labels (e.g., `--labels bug,frontend`) produces JSON files containing the defined schema and metadata.\r\n- JSON output includes `generated_at`, `repository`, `label`, and issue details (number, title, state, assignees, milestone, updated_at, html_url).\r\n- Tests covering JSON assembly and API wrapper logic are added and passing.\r\n- Documentation explains configuration, invocation, and output location.\r\n\r\n## Open Questions / Follow-ups\r\n- Should the script support pagination limits or a max-issue count?\r\n- Do we include closed issues by default?\r\n- Future enhancement: schedule via GitHub Actions once the base feature lands.\r\n",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Documentation] Add Codex prompt files per module",
    "body": "Create a folder called prompts/ with .md or .txt files for:\n\nBitbucket client\n\n- Jira client\n- Secrets resolution\n- Streamlit UI\n- Delta tracker\n\nThese prompts can be fed to Codex/ChatGPT later to regenerate or improve functionality. Promotes AI-compatible development practices.\n\nCodex Prompt:\nFor a Python project using AI-assisted dev (Codex), create a folder `prompts/` with:\n- prompt_bitbucket_client.txt\n- prompt_jira_client.txt\n- prompt_config_loader.txt\n- prompt_streamlit_ui.txt\nEach file should include a short natural-language prompt that describes the module's purpose and how to regenerate its code using Codex.\n",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Draft Streamlit UI skeleton",
    "body": "Create a minimal Streamlit app to load/export audits. The app should allow users to run the CLI audit via a button, view results in a table, and download the report.\n\nAcceptance Criteria:\n- [ ] app.py with a “Run Audit” button\n- [ ] Table view of summary statistics\n- [ ] “Download Excel/JSON” buttons",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "Research agent frameworks",
    "body": "Evaluate LangGraph and crewAI for modular agent orchestration. Identify how they can be used to structure Jira/Bitbucket/RAG agents and later integrate with MCP memory.\n\nAcceptance Criteria:\n- [ ] Write short comparison doc (pros/cons, integration strategy)\n- [ ] Create demo agent that wraps Jira client for queries\n- [ ] Recommend framework for future refactor",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[CI] Coverage Gate + PR Summary Comment",
    "body": "*Description:**  \nIntegrate pytest-cov into CI pipeline. Fail build if coverage < 70% initially (raise later). Use GitHub Actions to post PR comment with coverage % summary.\n\n**Codex Prompt:**  \nImplement pytest-cov integration in `.github/workflows/ci.yml`. Generate `coverage.xml` and upload with `actions/upload-artifact`. Add a step to parse coverage and post a PR comment.  \n\n**Acceptance Criteria:**  \n- CI fails if coverage < 70%.  \n- PRs show coverage summary as a comment.  ",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Unit Tests] Mocked Jira & Bitbucket Clients",
    "body": "**Description:**  \nAdd unit tests with `pytest` + `responses` (or `pytest-httpx`) to mock Jira/Bitbucket APIs. Ensure clients handle pagination, errors, and retries.\n\n**Codex Prompt:**  \nAdd tests under `tests/clients/` for Jira/Bitbucket API wrappers. Mock HTTP responses to validate pagination, retries, and error handling.  \n\n**Acceptance Criteria:**  \n- Tests pass without real API calls.  \n- Retry logic covered.  \n",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Integration Tests] End-to-End with Cached Payloads",
    "body": "*Description:**  \nRun E2E test of CLI audit using cached JSON payloads (no network). Verify Excel/JSON reports match expected schema.\n\n**Codex Prompt:**  \nAdd `tests/integration/test_audit_end_to_end.py`. Use sample cached payloads in `tests/fixtures/`. Validate outputs in `dist/`.  \n\n**Acceptance Criteria:**  \n- `pytest -q` passes without network.  \n- Outputs match schema snapshot.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Contract Tests] Exporter Schema Stability",
    "body": "**Description:**  \nPrevent breaking changes to JSON/Excel outputs. Add schema contract tests.\n\n**Codex Prompt:**  \nImplement `tests/contract/test_export_schema.py`. Compare generated JSON against baseline schema using `jsonschema`. For Excel, validate required columns.  \n\n**Acceptance Criteria:**  \n- Tests fail on incompatible schema change.  ",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Static Typing] mypy Baseline",
    "body": "**Description:**  \nIntroduce static typing checks with mypy. Start with `--ignore-missing-imports`. Fail CI on type errors in core modules.\n\n**Codex Prompt:**  \nAdd mypy config in `pyproject.toml`. Update `.github/workflows/ci.yml` to run `mypy src/`.  \n\n**Acceptance Criteria:**  \n- CI fails on new type errors.",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[CI Matrix] Python 3.10 & 3.11",
    "body": "**Description:**  \nEnsure compatibility with Python 3.10 and 3.11 via CI job matrix.\n\n**Codex Prompt:**  \nUpdate `.github/workflows/ci.yml` to define matrix strategy for Python 3.10 and 3.11. Run lint + pytest for both.  \n\n**Acceptance Criteria:**  \n- CI runs jobs on both Python versions. ",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[CI] Smoke Test for Codex Branches",
    "body": "**Description:**  \nAdd lightweight smoke-test workflow triggered by codex/** branches. Runs lint, tests, synth only. Cancels in-progress runs on same branch.\n\n**Codex Prompt:**  \nCreate `.github/workflows/smoke.yml`. Trigger on `codex/**` branches. Use `concurrency` group keyed to branch. Run lint/tests/synth.  \n\n**Acceptance Criteria:**  \n- Every codex/** push runs smoke checks.  \n- Old jobs canceled on new push.  ",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Uploader] Dry-Run Mode + Smoke Test",
    "body": "**Description:**  \nEnhance uploader CLI with `--dry-run`. Add CI smoke test to validate outputs without hitting AWS.\n\n**Codex Prompt:**  \nAdd `--dry-run` flag in CLI uploader. If enabled, print what would be uploaded but skip S3. Add CI test to validate behavior.  \n\n**Acceptance Criteria:**  \n- CLI shows dry-run log.  \n- CI test confirms no AWS calls made. ",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "SQS DLQ] for Webhook Lambda + Replay Utility",
    "body": "**Acceptance Criteria**  \n- SQS DLQ attached to webhook Lambda.  \n- Replay CLI/Lambda with metrics.  \n- CloudWatch alarm on DLQ depth.  \n- Runbook for replay steps.\n\n**Manual AWS Steps**  \n1. AWS Console → SQS → Create Queue → Standard. Name: `releasecopilot-dlq`.  \n2. In Lambda → Configuration → Asynchronous invocation → DLQ → select queue.  \n3. Test: push invalid event → check DLQ message.  \n4. Manually replay with AWS CLI.  \n\n**Codex Prompt**  \n“Attach DLQ to webhook Lambda + implement replay tool. Add alarms on DLQ depth and docs for replay.”",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[EventBridge] Phoenix-time Cron + Reconciliation Slot",
    "body": "**Acceptance Criteria**  \n- EventBridge rule fires nightly aligned to Phoenix time.  \n- Target wired to reconciliation Lambda.  \n- Dry run log proof.  \n- DST behavior noted.\n\n**Manual AWS Steps**  \n1. AWS Console → EventBridge → Rules → Create rule.  \n2. Choose Schedule → Cron expression for midnight Phoenix time.  \n3. Target = reconciliation Lambda.  \n4. Save and test run.  \n5. Check CloudWatch logs.  \n\n**Codex Prompt**  \n“Create nightly EventBridge schedule (Phoenix) to invoke reconciliation Lambda; log schedule metadata.”",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Metrics Dash] Ingestion & Reconciliation Dashboard",
    "body": "**Acceptance Criteria**  \n- CW dashboard with ingest, dedup, errors, DLQ depth, drift.  \n- Linked in README/docs.  \n- Saved as CDK construct.\n\n**Manual AWS Steps**  \n1. AWS Console → CloudWatch → Dashboards → Create.  \n2. Add widgets for Lambda metrics, DDB, DLQ.  \n3. Save dashboard.  \n4. Link in README.  \n\n**Codex Prompt**  \n“Create a CW dashboard for ingest/recon metrics; expose as CDK construct and link in docs.”",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Runbook] Webhook Ingest: On-call + Replay",
    "body": "**Acceptance Criteria**  \n- Steps: rotate secret, replay DLQ, force re-backfill.  \n- Stored in `docs/runbooks/`.  \n- References alarms + dashboards.  \n\n**Manual AWS Steps**  \n1. Draft runbook in Markdown.  \n2. Test by rotating webhook secret manually.  \n3. Walk through DLQ replay.  \n4. Document results.  \n\n**Codex Prompt**  \n“Write operator runbook for webhook ingest (secret rotation, DLQ replay, JQL slice backfill, health checks).”",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Bootstrap] Parameterized JQL Scopes + Checkpointing",
    "body": "**Acceptance Criteria**  \n- JQL scopes parameterized.  \n- Checkpoint persisted.  \n- Summary artifact created.  \n\n**Manual AWS Steps**  \n1. Ensure S3 bucket exists for checkpoint artifacts.  \n2. Run bootstrap job manually, break mid-run.  \n3. Restart, confirm resume.  \n4. Inspect checkpoint file.  \n\n**Codex Prompt**  \n“Enhance bootstrap job with param JQL scopes, checkpoint persistence, summary artifact, and interrupt/resume test.”",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "[Custom Domain/TLS] API GW Custom Domain + ACM",
    "body": "**Acceptance Criteria**  \n- ACM cert created.  \n- API GW custom domain + mapping.  \n- Route53 record.  \n- Doc on cert rotation.  \n\n**Manual AWS Steps**  \n1. AWS Console → ACM → Request cert for domain.  \n2. Validate DNS in Route53.  \n3. API Gateway → Custom Domain → Add mapping.  \n4. Add Route53 alias record.  \n5. Confirm HTTPS works.  \n\n**Codex Prompt**  \n“Add API Gateway custom domain with ACM cert + Route53 mapping; document cert rotation.”  ",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  },
  {
    "title": "feat(docs): Publish MkDocs Material site via GitHub Actions (GitHub Pages)",
    "body": "## Summary\nSet up an automated documentation pipeline that builds the MkDocs Material site from `docs/` on every push to `main` and publishes it to **GitHub Pages** using **GitHub Actions**. Target URL:\n\n- **https://dutchsloot84.github.io/ReleaseCopilot-AI/**\n\nThis keeps built assets out of `main`, gives us fast/consistent deploys, and makes it trivial for contributors to update docs by editing Markdown.\n\n---\n\n## Why this matters (Concepts)\n- **Single source of truth:** Docs live next to code and are auto-published to a stable URL.\n- **Frictionless authoring:** Contributors edit Markdown in `docs/` only; no local build steps required.\n- **Traceability:** Each deploy ties back to a commit and is visible in the Actions history.\n- **CI parity:** Same toolchain runs locally (`mkdocs serve`) and in CI (`mkdocs build`).\n\n---\n\n## Scope (what’s included)\n1. A GitHub Actions workflow at `.github/workflows/docs.yml` that:\n   - Installs Python + MkDocs Material (and common plugins),\n   - Builds the site into `site/`,\n   - Publishes to GitHub Pages with the official `actions/deploy-pages` flow.\n2. A minimal, working `mkdocs.yml` in the repo root.\n3. A starter `docs/` structure with at least `index.md` (and placeholders for CLI & Architecture pages).\n4. Project README update: add the public docs URL and a short authoring section (how to preview and contribute).\n\n---\n\n## Out of scope\n- Content writing beyond placeholders.\n- Versioned docs (we can add `mike` later).\n- Custom domain configuration (CNAME/DNS).\n\n---\n\n## Implementation plan (Practical steps)\n### 1) Create the GitHub Actions workflow\nCreate `.github/workflows/docs.yml`:\n```yaml\nname: docs\n\non:\n  push:\n    branches: [ main ]\n  workflow_dispatch:\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: true\n\njobs:\n  build:\n    name: Build MkDocs site\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.x\"\n\n      - name: Install MkDocs + theme\n        run: |\n          python -m pip install --upgrade pip\n          pip install mkdocs-material mkdocs-minify-plugin mkdocs-git-revision-date-localized-plugin\n          if [ -f requirements-docs.txt ]; then pip install -r requirements-docs.txt; fi\n\n      - name: Build\n        run: mkdocs build --strict\n\n      - name: Upload Pages artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: site\n\n  deploy:\n    name: Deploy to GitHub Pages\n    needs: build\n    runs-on: ubuntu-latest\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    steps:\n      - id: deployment\n        uses: actions/deploy-pages@v4\n```\n\n### 2) Add base `mkdocs.yml` to repo root\n```yaml\nsite_name: ReleaseCopilot-AI\nsite_url: https://dutchsloot84.github.io/ReleaseCopilot-AI/\nrepo_url: https://github.com/dutchsloot84/ReleaseCopilot-AI\nedit_uri: edit/main/docs/\n\ntheme:\n  name: material\n  features:\n    - navigation.instant\n    - content.code.copy\n    - search.suggest\n    - search.highlight\n\nnav:\n  - Home: index.md\n  - CLI: cli.md\n  - Architecture: architecture.md\n\nplugins:\n  - search\n  - minify\n  - git-revision-date-localized:\n      fallback_to_build_date: true\n```\n\n### 3) Seed `docs/` content\nCreate minimal files:\n```\ndocs/\n├─ index.md\n├─ cli.md\n└─ architecture.md\n```\nExample `docs/index.md`:\n```markdown\n# ReleaseCopilot-AI\n\nWelcome! This site documents ReleaseCopilot-AI — a tool that automates release audits by correlating Jira stories with Bitbucket commits and exporting results (JSON/Excel), with optional S3 publishing.\n\n## Quick links\n- CLI usage: [CLI](./cli/)\n- Architecture overview: [Architecture](./architecture/)\n\n## Contributing\nEdit Markdown under `docs/` and open a PR. To preview locally:\n\n```bash\npip install mkdocs-material\nmkdocs serve\n```\n```\n\n### 4) Configure Pages\n- Repo → **Settings → Pages**  \n  - **Source:** *GitHub Actions* (keep as-is).  \n- First publish occurs on the next push to `main` after the workflow is merged.\n\n### 5) README update\nAdd a **Documentation** section with:\n- Public URL: `https://dutchsloot84.github.io/ReleaseCopilot-AI/`\n- “How to edit docs” (point to `docs/`, `mkdocs serve`), and link to this Issue for the deploy workflow.\n\n---\n\n## Acceptance criteria\n- [ ] Workflow file exists at `.github/workflows/docs.yml` and passes in Actions.\n- [ ] `mkdocs.yml` present at repo root; `docs/index.md` renders without build errors.\n- [ ] Pages environment shows “Active” and the site is reachable at **https://dutchsloot84.github.io/ReleaseCopilot-AI/**.\n- [ ] README includes a Documentation section with the live URL and local preview instructions.\n- [ ] Screenshots or run link attached in PR comment confirming a successful Pages deployment.\n\n---\n\n## Risks & mitigations (Learning callout)\n- **Plugin mismatch** → Keep plugin list in sync with `mkdocs.yml`; install missing plugins in the workflow.\n- **Empty docs** → `mkdocs build --strict` fails if nav references missing files. Start minimal and grow.\n- **Pages mis-config** → Ensure “Source = GitHub Actions” and environment is created by the first successful deploy.\n- **Cache busting** → Material already fingerprints assets; no special headers needed for Pages.\n\n---\n\n## Testing & verification\n- Local: `pip install mkdocs-material && mkdocs serve` → open `http://127.0.0.1:8000`.\n- CI: Confirm the **docs** workflow is green and that the `Deploy to GitHub Pages` step reports a public URL.\n- Browser: Load the site, use search, and click “Edit this page” to verify repo linkage.\n\n---\n\n## Rollback\n- Revert the workflow commit or disable the **docs** workflow in Actions.\n- Pages will retain the last successful artifact; disabling the workflow stops future publishes.\n\n---\n\n## Developer prompt for Codex (generate PR changes)\n> Paste the following into Codex (or your LLM assistant) to create the branch, files, and PR.\n\n```\nYou are an expert GitHub + MkDocs automation assistant.\n\nGoal:\nImplement automated MkDocs Material docs publishing for the repo \"ReleaseCopilot-AI\" using GitHub Actions → GitHub Pages, with minimal configuration and a working starter docs set.\n\nDeliverables (create in a new branch docs/gh-pages-pipeline):\n1) Add `.github/workflows/docs.yml` exactly as specified in the Issue.\n2) Add a base `mkdocs.yml` at the repo root exactly as specified in the Issue.\n3) Create `docs/index.md`, `docs/cli.md`, and `docs/architecture.md` with minimal placeholder content.\n4) Update `README.md` to include:\n   - Documentation URL: https://dutchsloot84.github.io/ReleaseCopilot-AI/\n   - Local preview instructions: `pip install mkdocs-material && mkdocs serve`\n   - A one-liner about the deploy pipeline (GitHub Actions → Pages).\n\nBranch & PR:\n- Create branch `docs/gh-pages-pipeline`.\n- Commit changes with messages:\n  - \"ci(docs): add MkDocs → GitHub Pages workflow\"\n  - \"docs: add mkdocs.yml and seed docs/ content\"\n  - \"docs(readme): link published docs + local preview instructions\"\n- Open a PR titled: \"docs(ci): publish MkDocs site to GitHub Pages via Actions\"\n- Include a checklist referencing the Acceptance Criteria from the Issue.\n- After merging, push a trivial change to `docs/index.md` to confirm an automatic redeploy.\n\nNotes:\n- Keep dependencies minimal. Do not introduce Poetry or lock files for docs-only.\n- If a `requirements-docs.txt` exists, install it in the workflow in addition to mkdocs-material.\n- Ensure the workflow uses `actions/deploy-pages@v4` with the Pages permissions enabled.\n```\n\n---\n\n## Historian markers\nDecision:\n- Use GitHub Actions (build) → GitHub Pages (deploy) with `actions/deploy-pages@v4` to publish MkDocs Material from `docs/` on pushes to `main`.\n\nAction:\n- (Owner: Shayne) Merge the PR that adds `.github/workflows/docs.yml`, `mkdocs.yml`, and seed `docs/` files; then verify the site is live at the expected URL.\n\nNote:\n- Versioned docs (via `mike`) and custom domain (CNAME) are deferred; keep scope minimal to land a working pipeline first.\n",
    "url": null,
    "status": "Backlog",
    "updatedAt": null
  }
]
